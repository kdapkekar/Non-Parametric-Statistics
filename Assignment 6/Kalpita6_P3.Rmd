---
title: "Assignment6_P3"
author: "KR"
date: "4/29/2020"
output: html_document
---

1.	Build a regression tree for Chicago Homes training data set and use it to predict the list prices of homes in the Chicago Homes test set.  Again do not use ZIP code

Load the datasets (ChiHomes(train).csv and ChiHomes(test).csv) and remove ZIP. 
```{r}
CH.train <- read.csv("D:/SPRING 2020/581/Assignment 6/ChiHomes(train).csv")
CH.test <- read.csv("D:/SPRING 2020/581/Assignment 6/ChiHomes(test).csv")
CH2.train<-CH.train[,-3]
CH2.test<-CH.test[,-3]
```

1. a) Look at the data in the training set:
```{r}
library(psych)
pairs.panels( CH2.train, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
)
```

1.b) Look at List price by itself (plot its histogram) and add a density curve to the histogram. By defaul, histogram plots frequencies. So you need to indicate probability=TRUE if you are going to add a density plot to the histogram.
```{r}
hist(CH2.train$ListPrice,probability = TRUE)
lines(density(CH2.train$ListPrice))
```
Now do the same to log of ListPrice

```{r}
hist(log(CH2.train$ListPrice),probability = TRUE)
lines(density(log(CH2.train$ListPrice)))
```
Comment on the distribution of ListPrice vs log(ListPrice). Why would one want to develop a model with log(ListPrice) instead of ListPrice?

Answer: The distribution of log(ListPrice) is closer to a normal distribution.


1.c) We will build the model then for log(ListPrice).

Build an initial model for log(ListPrice) using defaul values for rpart, and plot it (use rpart.plot)
```{r}
library(rpart)
rp <- rpart(log(CH2.train$ListPrice)~ .,data=CH2.train)
summary(rp)

library(rpart.plot)
rpart.plot(rp)
```

1.c.i) What are the 3 most important variables?
ImputedSQFT, BATHS and BEDS.
```{r}
rp$variable.importance
```

1.c.ii) What is the prediction accuracy of this default model? Be careful here because we have used log(ListPrice), 
```{r}
logpred<-predict(rp,newdata = CH2.test)
library(spm)
PredAcc(log(CH2.test$ListPrice),logpred)   
```

2.	Now use the training data  to tune your RPART model using cp and  minbucket (or minsplit) to improve the model.

```{r}
rp <- rpart(log(CH2.train$ListPrice)~.,data=CH2.train,control=rpart.control(xval=10,cp=0.0001,minbucket = 10))
```
Visualise x-val results as a function of the complexity parameter using plotcp(rp) 

```{r}
plotcp(rp) #
```

2.a) Make a judgement call to choose cp
Answer: cp = 0.0014

2.b) make your model with the chosen value for cp and check the predicted accuracy (PredAcc from notes)
```{r}
rp2 <- rpart(log(CH2.train$ListPrice)~ .,data=CH2.train,control=rpart.control(xval=10 ,cp=.0014))
PredAcc(log(CH2.test$ListPrice),predict(rp2,newdata=CH2.test)) 
```

2.c) Plot your model with rpart.plot
```{r}
rpart.plot(rp2)
```

2.d) plot log(ListPrice) vs its predicted value and comment on your findings. Is it reasonable mosdel?
```{r}
plot(log(CH2.test$ListPrice),predict(rp2,newdata=CH2.test),
xlab="Actual values",ylab="Predicted values")
abline(0,1,lwd=3,col="blue")
```
Comment: Yes, it is reasonable model.


3.	

Run the following code for the function below.
```{r message = FALSE}
tree.vary = function(fit,data=CH2.train) {
    n = nrow(data)
    sam = sample(1:n,floor(n*.5),replace=F)
    temp = rpart(formula(fit),data=data[sam,])
    prp(temp,type=4,digits=3,roundint = FALSE)
}
```

```{r}
par(mfrow=c(2,2))
for(i in 1:4){
tree.vary(rp2,data=CH2.train)
}
```
```{r}
attach(diamonds)
rp3 <- rpart(price ~ carat ,data=diamonds,control=rpart.control(xval=10 ,cp=0.0016))

tree.vary = function(fit,data = diamonds) {
n = nrow(data)
sam = sample(1:n,floor(n*.5),replace=F)
temp = rpart(formula(fit),data=data[sam,])
prp(temp,type=4,digits=3,roundint = FALSE)
}

par(mfrow=c(2,2))
for(i in 1:4){
tree.vary(rp3,data=diamonds)
}
```


3.a) Look at each tree as it is plotted, what do you notice?

 i. Price increases with the carat.
ii. Carat (<0.995 & >=0.995) group appears in all the 4 trees that means we can see the differnce in the prices making it better cut.

3.b)  Build a bagged regression tree for Chicago Homes training data set and use it to predict the list prices of homes in the Chicago Homes test set.

```{r}
library(ipred)
CH.bag = bagging(log(CH2.train$ListPrice)~.,data=CH2.train,coob=T,nbagg=50,
                   control=rpart.control(cp=.002,minsplit=2,xval=0) )
```

Find the predicted accuracy of the bagged model:
```{r}
PredAcc(log(CH2.test$ListPrice),predict(CH.bag,CH2.test))
```
How does it compare with the model developed in 2.c) ? 

Answer: RMSE value is less than the RMSE acquired in 2.c). It predicts better than model RPART 



4. Build a random forest using Chicago Homes training data set and then predict the list prices of homes in the Chicago Homes test set.  Again we will not use ZIP code in the modeling process.

Build the model:
```{r message = FALSE}
library(randomForest)
CH.rf<-randomForest(log(ListPrice)~., data=CH2.train)
CH.rf
```

4.a) Find the most important variables by plotting:
```{r}
varImpPlot(CH.rf)
```
4.a.i) Which two variables are candites to be dropped from the model?
Ans : SoldPrev and BeenReduced.

4.a.ii) How does log(ListPrice)  change as a function of the top four numeric predictors?  Do the partial plots on the 4 leading variables:
```{r}
par(mfrow=c(2,2))
partialPlot(CH.rf,CH2.train,ImputedSQFT)
partialPlot(CH.rf,CH2.train,BATHS)
partialPlot(CH.rf,CH2.train,BEDS)
partialPlot(CH.rf,CH2.train,Type)
par(mfrow=c(1,1))
```

4.b) Find the model's predictive accuracy  (use PredAcc)
```{r}
 PredAcc(log(CH2.test$ListPrice),predict(CH.rf,CH.test))
```
How does it compare to the models in 2.c) and 3.b)
Ans: Again the RMSE value is less than the RMSE values of both the model, it is better than previous bagging and RPART.


